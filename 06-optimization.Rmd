# Optimization

Much of statistical and machine learning boils down to function optimization (if you are not necessarily interested in Bayesian inefrence). In you don't want to implement an optimizer for yourself, the following couple of libraries might be of interest to you.

## Convex optimization

`CVXR` is a package for discplined convex optimization. As log as you can formulate your objective following the conventions from the package, `CVXR` automatically verifies convexity and chooses a solver. As an example, below we compare linear regression models solved using OLS and Huber loss:

```{r}
library(CVXR)
library(lme4)

x <- rnorm(100)
y <- 1 + 2 * x + rnorm(100)
beta <- CVXR::Variable(1)

obj.ols      <- CVXR::sum_squares(y - x %*% beta)
result.ols   <- solve(CVXR::Problem(CVXR::Minimize(obj.ols)))
obj.huber    <- CVXR::sum_entries(CVXR::huber(y - x %*% beta, 1))
result.huber <- solve(CVXR::Problem(CVXR::Minimize(obj.huber)))

beta.hat.ols   <- result.ols$getValue(beta)
beta.hat.huber <- result.huber$getValue(beta)

beta.hat.ols
beta.hat.huber
```

TODO more examples and references

## Non-linear optimizaton

In cases, where your objective is not differentiable, you have box-constraints, or whatsoever `nloptr` is an excellent choice for function minimization. Following the example from above:

```{r}
library(nloptr)

ols <- function(x, m, n) {
  0.5 * sum((m - n * x)^2)
}

huaba <- function(x, m, n) {
  thresh <- sum(abs(m - n * x))
  if (thresh <= 1)
    ols(x, m, n)
  else
    thresh - 0.5
}

result.hat.ols <- nloptr::nloptr(
  1, ols, lb = -10, ub = 10, 
  opts=list("algorithm" = "NLOPT_LN_SBPLX", maxeval=1000), m=y, n=x)
result.hat.huaba <- nloptr::nloptr(
  1, huaba, lb = -10, ub = 10, 
  opts=list("algorithm" = "NLOPT_LN_SBPLX", maxeval=1000), m=y, n=x)

result.hat.ols$solution
result.hat.huaba$solution
```

TODO more examples and references
