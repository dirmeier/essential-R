# Data analysis and statistical modeling

The following few sections introduce packages I frequently use for data analysis, modelling and machine learning.

## Stan

`Stan` is a great package for Bayesian modelling and inference. Compared to statistical packages that are fit to one model, such as `glmnet` or `lme4`, Stan allows to easily define custom Bayesian models for which posterior distributions are automatically inferred using HMC. 

A simple liner regression model could look like this:
```{r}
library(rstan)
library(lme4)

model <- "
data {
  int<lower=1> n;
  vector[n] x;
  vector[n] y;
}

parameters {
  real beta;
  real<lower=0> sigma;
  real alpha;
}

model {
  beta ~ normal(0, 5);
  sigma ~ cauchy(0, 5);
  y ~ normal(alpha + x * beta, sigma);
}
"

n <- nrow(sleepstudy)
x <- sleepstudy$Days
y <- sleepstudy$Reaction

fit <- stan(model_code=model, data=list(n=n, x=x, y=y), 
            warmup=100, iter=1100, chains=1)
summary(fit)
```

### Rstanarm

`rstanarm` is a package for applied Bayesian modelling that wraps around Stan for easier usage. 

```{r}
library(rstanarm)

fit.arm <- rstanarm::stan_glm(
  Reaction ~ Days, sleepstudy, chains = 1,
  iter = 1100, warmup = 100, family = gaussian())
summary(fit.arm)
```

### brms

For non-linear multi-level models `brms` is also a great option. 

```{r}
library(brms)

fit.multilevel <- brm(Reaction ~ Days + (Days | Subject), sleepstudy, chains=1, iter=1000)
summary(fit.multilevel)
```

### bayesplot

For plotting of Bayesian models inferred using the tools mentioned above, you probably want to use `bayesplot`. For instance, to compare posterior predictive intervals between the `rstanarm` linear model and the mixed model form `brms`:

```{r, fig.width=10, fig.height=4}
library(bayesplot)
library(cowplot)

p1 <- ppc_intervals(y = sleepstudy$Reaction,
              yrep = posterior_predict(fit.arm)) +
  labs(title="Linear model")
p2 <- ppc_intervals(y = sleepstudy$Reaction,
                    yrep = posterior_predict(fit.multilevel)) +
  labs(title="Linear mixed model")

cowplot::plot_grid(p1, p2, ncol=2, align="h")
```

## greta

Model definitions in Stan are arguable difficult in the beginning. As an alternative with `greta` is not only easier to compose models, but also often faster owing to the fact that it's developed against tensorflow. Installation can be a bit tedious. For `greta` version `v.0.3` I would install tensorflow from the command line:

```{bash, eval=FALSE}
conda create -y -n r-tensorflow python=3.6
conda install tensorflow==1.10.0
pip install tensorflow-probability==0.30.0
```

The same model we used for stan above, looks like this with greta:

```{r, eval=FALSE}
library(greta)
library(lme4)

n <- nrow(sleepstudy)
x <- sleepstudy$Days
y <- sleepstudy$Reaction

alpha <- variable()
beta <- greta::normal(0, 5)
sigma <- greta::cauchy(0, 5, truncation=c(0, Inf))

y <- greta::as_data(y)
greta::distribution(y) <- greta::normal(alpha + x * beta, sigma)

mod <- greta::model(beta)
samples <- greta::mcmc(mod, n_samples=1000, warmup=100, chains=1)
```

## MCMC

There are a couple of packages on CRAN especially for Markov Chain Monte Carlo and Bayesian methods, some of which are mentioned below.

`coda` is a package for analysis and diagnostics of MCMC chains. Mostly it takes arguments of class mcmc.list, so put your results into an object of it to be able to use `coda`:
```{r}
library(coda)

coda::autocorr(As.mcmc.list(fit))
coda::traceplot(As.mcmc.list(fit))
```

`MCMCpack` offer an alternative to rstanarm for applied Bayesian modelling. It also comes with a set of useful distributions used frequently in Bayesian modelling, such as the Dirichlet, inverse gamma, etc. 

```{r}
library(MCMCpack)
invg <- MCMCpack::rinvgamma(1000, 5, 1)
hist(invg, breaks=50, xlab="X", main="", col="darkgrey", family="serif")
```

For plotting `mcmc.list` objects (from `coda`) `MCMCvis` is great:

```{r}
MCMCvis::MCMCplot(As.mcmc.list(fit))
```

## mlR and openML

CRAN offers dozens of packages related to machine and statistical learning, many of which doing the same. `mlR` wraps many of these into one big library.
`mlR` integrates with `openML`, an open machine learning platform where people share code, data and algorithms. Here we show an example where we use a Gaussian process to predict the Kyphosis label from the `gam` package.

```{r}
library(mlr)
task  <- mlr::makeClassifTask(data = kyphosis, target = "Kyphosis")
lrn   <- mlr::makeLearner("classif.gausspr")

n <- nrow(kyphosis)
train.set <- sample(n, size = 2/3*n)
test.set  <- setdiff(1:n, train.set)

model <- mlr::train(lrn, task, subset = train.set)
pred  <- stats::predict(model, task = task, subset = test.set)
performance(pred, measures = list(mmce, acc))
```

## Tensorflow

Thanks to Rstudio, R users are able to use `tensorflow`, a library for high performance numerical computations (which for instance greta uses). For instance, a linear model could look like this:

```{r, eval=FALSE}
library(tensorflow)

n <- nrow(sleepstudy)
x <- sleepstudy$Days
y <- sleepstudy$Reaction

# define model
beta  <- tf$Variable(tf$random_normal(shape(1L), 0, 10))
alpha <- tf$Variable(tf$zeros(shape(1L)))
y.hat <- alpha + beta * x

# Minimize the mean squared errors.
loss <- tf$reduce_mean((y - y.hat) ^ 2)
optimizer <- tf$train$GradientDescentOptimizer(.5)
train <- optimizer$minimize(loss)

# Launch the graph and initialize the variables.
sess <- tf$Session()
sess$run(tf$global_variables_initializer())

for (step in 1:100)
{
  sess$run(train)
}
```

## Keras

`Keras` is an interface to popular numerical libraries such as tensorflow and theano for which model/network definitions are independent of the library on the backend.
Our tensorflow model from above would look the following in keras:

```{r, eval=FALSE}
library(keras)

model <- keras_model_sequential() %>%
  layer_dense(units = 1, activation = "linear", input_shape = 1)
model %>%
  compile(loss = "mse",
          optimizer = optimizer_sgd(0.5),
          metrics = list("mean_absolute_error"))
```

## Statistical learning

Some packages for regression:

* `glmnet` for $\ell_1$- and $\ell_2$-penalized linear regression models,
* `lme4` for frequentist mixed models,
* `mgcv` for generalized additive models,
* `netReg` for graph regularized linear models,
* `xgboost` and `gbm` for boosting.
* `h20` for general machine learning algorithms,

TODO some examples

## Big data analytics

For Big data analytics I recommend Rstudio's `sparklyr` since it nicelt itegrates with the other methods from the `tidyverse`. For instance, following an example from [Rstudio's tutorials](https://spark.rstudio.com/mlib/):

```{r, eval=FALSE}
library(sparklyr)

sc <- spark_connect(master = "local")
kmeans_model <- copy_to(sc, iris, "iris", overwrite = TRUE) %>%
  select(Petal_Width, Petal_Length) %>%
  ml_kmeans(centers = 3)
```

## Others

Some other great packages for various data-related things:

### modelr

`modelr` defines multiple helper functions related to statistical modelling:

```{r}
library(modelr)

sleepstudy %>% modelr::fit_with(
  lm, 
  modelr::formulas(~Reaction,
                   no_intercept = ~0 + Days, 
                   intercept = ~1 + Days))
```

### kernlab

Use `kernlab` for tasks related to kernels, such as Gaussian process regression or merely computing a Gram-matrix.

```{r}
library(kernlab)

x <- matrix(rnorm(25), 5)
rbf <- kernlab::rbfdot()
(K <- kernlab::kernelMatrix(rbf, x))

x <- sort(rnorm(100))
y <- 3 + 0.5 * x^2 + 1 * x + rnorm(100, 0, 0.05)
gpr <- kernlab::gausspr(x, y)
plot(x, y, family="serif")
lines(x, predict(gpr, x), col="red")
```
